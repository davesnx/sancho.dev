---
title: "Safer languages might be the future of AI-driven development"
publishedAt: "2025-10-12"
isDraft: true
tags:
  - "AI"
  - "LLMs"
  - "Programming Languages"
  - "Type Safety"
description: "Why LLMs might benefit from maximally-safe programming languages, and why even current languages aren't enough"
---

The rapid evolution of LLMs is reshaping software development. We see them assisting in code generation, debugging, and even architectural design. But there's something odd happening: ask an LLM to solve a problem (without specifying the language), and it almost always reaches for Python or TypeScript.

This isn't surprising. As [Kiran Gopinathan notes](https://kirancodes.me/posts/log-lang-design-llms.html), LLMs have substantially higher efficacy when operating in languages well-represented in their training data.

Python and TypeScript are the most used programming languages, so LLMs know them best. But here's the question that's been nagging me: **is this the optimal direction?**, **does training data be the only factor, as LLMs get better**? or even looking further, **what would be a language made for LLMs**?

### Why they are the most common?

Both languages share the same philosophy: remove friction to let developers move fast. Low barrier to entry, instant feedback, no compilation step. This permissiveness is exactly why they spread everywhere (PHP and Ruby are great examples too).

Python became the language of scripting, then data science, then machine learning. JavaScript became the only language that runs natively in browsers, making it unavoidable for web development.

In this blog post I want to reconsider the languages we choose in this new era. This may not be about LLMs adapting to our current paradigms, and whether certain language characteristics make them more suitable for AI-driven development.

I'll spoil it upfront: even as a bit OCaml fan, I've firmly believe it might not be the best choice neither. Mostly because **not being safe enough**.

## The shift was already happening

Even before LLMs, the industry was already moving toward type-safe languages. As your codebases grow, the cost of fixing bugs scales dramatically.

 Type-safe languages eliminate the moment where you fix one bug and four pop up. They catch entire classes of errors at compile-time, making refactoring a pleasure instead of a pain and allowing more reasoning about the code.

This was happening independently in a lot of ecosystems at the same time:

- **TypeScript** brought gradual static typing to JavaScript, and it's now the default for serious frontend work
- **Elixir** added an aclamed type system enhancements, bolstering its reputation for fault-tolerant systems
- **Gleam** emerged with strong static typing for the Erlang VM
- Even **Python** or **PHP** now has type hints baked into the language

## Compilers as feedback loop

A strict compiler tells you immediately when something is logically inconsistent. This **instant feedback** is exactly [what helps LLMs run longer and achieve better results](https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents). The compiler errors/successes guide them toward correctness with every iteration: [improving LLM accuracy by over 80%](https://arxiv.org/abs/2403.16792).

My thesis is: the safer the language, the more the compiler can verify, the faster the feedback loop, and the more reliably the LLM converges on correct code. This will compound.

[Yaron Minsky recently asked](https://x.com/yminsky/status/1980620162144378957) whether anyone had studied AI agent efficacy across statically vs dynamically typed languages. The replies were telling. One developer noted that agents perform better with TypeScript than JavaScript because "they have access to the linter, so they can fix their own typing mistakes." Another found AI outperforming expectations in Rust "due to helpful compiler messages related to the type system and borrow checker."

The pattern is consistent: agents aren't just benefiting from training data—they're benefiting from tooling that catches their mistakes. Yaron's own response captured it well: one-shotting a simple program is one thing, but "making changes in a complex codebase is hard, and having strong tooling to inform agentic flows is really important."

I now include a rule in all my projects: "Never accept broken compiler output. Every type mismatch must be fixed. When in doubt, ask me." It works extremely well.

## The safety spectrum

I mentioned "safety" in a lot of places, but what does even mean?

Not all safety is equal. [Different languages verify different properties](https://cdsmith.wordpress.com/2011/01/09/an-old-article-i-wrote/). Think of it as a spectrum of "what can the compiler can know from your code?"

**Type safety** ensures operations are valid for the data they operate on. Languages like OCaml and Gleam excel here, their compiler enforces contracts between functions, makes illegal states unrepresentable, and guarantees exhaustive pattern matching.

**Memory safety** catches buffer overflows, use-after-free errors, and dangling pointers. Rust is the gold standard, using ownership and borrowing to guarantee memory safety at compile time without a garbage collector. [Microsoft found that 70% of their CVEs are memory safety issues](https://www.microsoft.com/en-us/msrc/blog/2019/07/we-need-a-safer-systems-programming-language).

**Thread safety** catches race conditions and data races. Again, Rust leads here with its ownership model preventing concurrent mutable access. Worth mention for OxCaml.

**Effect tracking** catches side effects. Haskell forces you to encode I/O operations in types, making it impossible to accidentally perform side effects in pure functions.

**Dependent types** catch logical properties. Even not being often classified as a safety feature, some languages like Idris and Lean let you express invariants in your types, proving properties about your code mathematically.

Each level up this spectrum means more properties the compiler can verify. More verification means tighter feedback loops for LLMs. An LLM working with a dependently-typed language could, in theory, prove its code correct rather than just hoping it works.

## Why systems beat discipline

Here's a truth most developers learn the hard way: **software tends toward chaos**. This is entropy.

You can write clean code. You can establish conventions. You can review every pull request. But over time, small decisions pile up. A waterfall here. A cache miss there. An abstraction that made sense when written but obscures critical details six months later. [The Broken Windows theory](https://yenkel.dev/posts/a-tale-of-latency-and-broken-windows) applies to code: if you let small issues slide, bigger ones follow.

The problem isn't incompetence. [Shu Ding captured it perfectly](https://x.com/shuding/status/2013632751568851233) after seven years of performance work at Vercel: "The same mistakes kept appearing. Different engineers. Different codebases. Different times." He concluded that performance degradation isn't a technical problem—it's entropy.

**Context doesn't scale.** Modern codebases grow faster than any individual can track. Dependencies, state machines, async flows, caching layers. You cannot hold the design of the entire system in your head while laying a single brick. Engineers make locally reasonable decisions that are globally suboptimal, not because they're careless, but because they lack the full picture.

The solution isn't more discipline. You cannot fight entropy with willpower.

The solution is **systems that enforce what discipline cannot**. This is exactly what strict compilers do. A type system doesn't ask you to remember which functions are pure—it tracks effects for you. A borrow checker doesn't rely on you to avoid data races—it makes them impossible. The compiler holds the context you can't.

And now we have something new: LLMs that can reference the entire design of the cathedral while examining every brick. They don't get tired. They don't lose context between sessions. Combined with a strict compiler, they form a system that fights entropy in ways humans alone never could.

This is why safer languages matter beyond just "catching bugs." They're not just feedback loops for correctness—they're bulwarks against the natural decay of software systems.

## The paradox: tedious for humans, perfect for machines

If safer languages are so powerful, why aren't we all writing Lean or Idris?

Because **safer languages are tedious to write**. They demand more attention:

- More learning curve
- More attention to types and their precise definitions
- More attention to memory ownership and lifetimes
- More attention to effect boundaries and purity
- More attention to proving properties hold

This is painful. When you're trying to ship a feature, fighting with a borrow checker or satisfying a type constraint feels like overhead. Sometimes as humans we want to "just make it work".

But here's the thing: **LLMs don't care about tedium**.

What's painful for us is trivial for them. An LLM can happily generate verbose type annotations, satisfy lifetime constraints, and write proof terms. It doesn't get frustrated. It doesn't cut corners because it's tired. The ceremony that makes safer languages feel heavy to humans is just more tokens to an LLM.

The situation where we found type-safety being painful is when you're migrating from unsafe to safe, or maybe when you're creating software from scratch. But it's marvelous when you're maintaining, refactoring, testing, and explaining code. LLMs could handle the painful part while we reap the benefits.

## Most languages aren't safe enough

Here's where I have to be honest about my favorite language.

OCaml has an excellent type system. It catches a huge class of errors at compile time. Pattern matching is exhaustive. The module system is powerful. I love working in it.

But for the purpose of maximizing compiler feedback for LLMs, OCaml hits a ceiling. There are things you simply can't express in its type system:

- **Memory safety**: OCaml has a garbage collector, which helps, but it doesn't have Rust's guarantees about ownership and borrowing. You can still create logical races with mutable state.
- **Race-free data operations**: There's no ownership model preventing concurrent access to shared mutable data.
- **Effect tracking**: Side effects can happen anywhere. The compiler doesn't know which functions do I/O and which are pure.
- **Dependent types**: You can't encode complex invariants in types. The type system, while strong, can't prove arbitrary properties about your code.

## But ecosystems matter

In TypeScript land, you can probably set up a Supabase database, BetterAuth authentication, Stripe payments, Twilio SMS, and Sentry error tracking in a single prompt. The LLM knows these APIs inside out. It's seen thousands of examples.

Now imagine doing the same in Rust, or even Lean. The LLM would struggle, not because the language is harder, but because there's less training data, fewer examples, smaller ecosystems. The libraries might not even exist.

This is the real barrier. Safer languages don't just need better compilers. They need ecosystems rich enough that LLMs can learn from them. Today, TypeScript wins not because it's the safest choice, but because it's the most documented one.

But again, I doubt the barrier of creating those nice abstractions is what's stopping us from choosing another "better" language (and sorry for the better, there's no better one!).

## The ideal language doesn't exist yet

What would the perfect language for LLM-assisted development look like? I think it's a combination that doesn't quite exist, let me enumerate a bunch of features:

**Rust's memory safety**: Ownership and borrowing, compile-time guarantees about memory access, no data races.

**Haskell's purity**: Effects tracked in types. The compiler knows exactly which functions can do I/O, modify state, or throw exceptions.

**Lean's dependent types**: Express arbitrary properties in your types. Prove that your sorting function actually sorts, that your parser handles all inputs, that your state machine only allows valid transitions.

Some languages are moving in this direction. Idris combines dependent types with effect tracking. Koka has algebraic effects. Rust keeps adding more expressive type features. But we're not there yet.

Research is already exploring this space. [CoCoGen](https://arxiv.org/abs/2403.16792) shows that compiler feedback can improve LLM code accuracy by over 80%. [VeCoGen](https://arxiv.org/abs/2411.19275) combines LLMs with formal verification, iteratively refining code until it satisfies specifications. [TheoremLlama](https://arxiv.org/abs/2407.03203) trains LLMs to write Lean proofs, using the type checker as a feedback mechanism.

We're in early days. The language that combines all these properties in a way that's practical for LLM-assisted development doesn't exist yet. But someone will build it.

## The future might be weirder than we think

Here's my speculation: LLMs might shift which languages succeed, but not in the way most people expect.

The conventional wisdom is that LLMs will entrench Python and JavaScript because that's what they know best. And in the short term, that's probably true.

But in the long term, the economics might flip. If LLMs can handle the tedium of safer languages, and if those languages provide better feedback loops, then the "difficult" languages become the productive ones. The languages humans find annoying might be exactly what LLMs prefer.

Imagine a future where you describe what you want in natural language, and an LLM writes Lean code that proves its own correctness. The code is verbose, heavily annotated, full of proof terms. You would never want to write it by hand. But you don't have to. The LLM handles the ceremony, and you get mathematically verified software.

That future isn't here yet. Today's LLMs [still get stuck in loops](https://zed.dev/blog/why-llms-cant-build-software), repeat mistakes, and need manual intervention. The safer languages have smaller communities and less training data. But the trajectory seems clear.

Maybe we should stop asking "what languages are LLMs good at?" and start asking "what languages would make LLMs better?" The answer might not be Python.
