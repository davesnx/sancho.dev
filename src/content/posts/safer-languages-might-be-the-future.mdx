---
title: "Safer languages might be the future of AI-driven development"
publishedAt: "2025-10-12"
isDraft: true
tags:
  - "AI"
  - "LLMs"
  - "Programming Languages"
  - "Type Safety"
description: "Why LLMs might benefit from maximally-safe programming languages, and why even OCaml isn't safe enough"
---

The rapid evolution of LLMs is reshaping software development. We see them assisting in code generation, debugging, and even architectural design. But there's something odd happening: ask an LLM to solve a problem, and it almost always reaches for Python or TypeScript.

This isn't surprising. As [Kiran Gopinathan points out](https://kirancodes.me/posts/log-lang-design-llms.html), LLMs have substantially higher efficacy when operating in languages well-represented in their training data. Python and JavaScript are everywhere, so LLMs know them best. But here's the question that's been nagging me: **is this the right direction?**

Python and JavaScript have been created for fast development, no constraints and it spreaded everywhere.

If we want to ship fast without breaking things, we might unlock more potential from LLMs by reconsidering the languages we use. This isn't about LLMs adapting to our current paradigms. It's about whether certain language characteristics make them inherently more suitable for AI-driven development.

I'll spoil it upfront: even as an OCaml fan, I've come to believe it might not be the best choice for LLMs. Not because it's too safe, but because it's **not safe enough**.

## The shift was already happening

Before LLMs entered the picture, the industry was already moving toward safer languages. This wasn't a coordinated effort. It was a response to a painful reality: as software systems grow in complexity, the cost of fixing bugs and vulnerabilities scales dramatically, while development speed slows down.

Developers and organizations recognized that languages preventing entire classes of errors at compile-time lead to more maintainable, secure, and reliable software. This proactive approach to error prevention was gaining momentum independently.

We've witnessed this shift across the ecosystem:

- **TypeScript** brought gradual static typing to JavaScript, and it's now the default for serious frontend work
- **Rust** gained significant traction for systems programming, offering memory safety without a garbage collector and robust concurrency guarantees
- **Elixir** added type system enhancements, bolstering its reputation for fault-tolerant systems
- **Gleam** emerged with strong static typing for the Erlang VM and JavaScript runtimes
- Even **Python** now has type hints baked into the language

This gravitation toward safer languages was, and continues to be, driven by practical necessity. The introduction of LLMs into this ecosystem isn't just a new toolset. It's an interaction with a paradigm that already values the very safety and predictability that LLMs themselves might benefit from.

## Compilers as feedback loops

Here's the core insight: **LLMs learn from feedback, and compilers provide instant, deterministic feedback.**

When you or I write code, we iterate. We try something, see if it works, fix errors, and try again. The tighter this feedback loop, the faster we improve. LLMs work the same way. They generate code, receive feedback, and adjust.

Now consider what different languages offer as feedback:

In Python, you write code, run it, and maybe it crashes at runtime. Maybe it silently does the wrong thing. The feedback is delayed and incomplete. You might not discover a bug until it hits production.

In a language with a strong type system, you write code, and the compiler immediately tells you if something is wrong. Not just "this will crash" but "this is logically inconsistent." The feedback is instant and comprehensive.

A strict compiler is like a patient teacher who never gets tired of saying "that's wrong, try again." For an LLM that can iterate thousands of times per second, this is powerful. The safer the language, the more the compiler can verify, the faster the feedback loop, and the more reliably the LLM can converge on correct code.

## The safety spectrum

Not all safety is equal. [Different languages verify different properties](https://cdsmith.wordpress.com/2011/01/09/an-old-article-i-wrote/). Think of it as a spectrum of "what can the compiler catch?"

**Type safety** catches operations on wrong data types. TypeScript and OCaml excel here. You can't accidentally pass a string where a number is expected.

**Memory safety** catches buffer overflows, use-after-free errors, and dangling pointers. Rust is the gold standard, using ownership and borrowing to guarantee memory safety at compile time without a garbage collector.

**Thread safety** catches race conditions and data races. Again, Rust leads here with its ownership model preventing concurrent mutable access.

**Effect tracking** catches side effects. Haskell forces you to encode I/O operations in types, making it impossible to accidentally perform side effects in pure functions.

**Dependent types** catch logical properties. Languages like Idris and Lean let you express invariants in your types, proving properties about your code mathematically.

Each level up this spectrum means more properties the compiler can verify. More verification means tighter feedback loops for LLMs. An LLM working with a dependently-typed language could, in theory, prove its code correct rather than just hoping it works.

## The paradox: tedious for humans, perfect for machines

If safer languages are so powerful, why aren't we all writing Coq?

Because **safer languages are tedious to write**. They demand more attention:

- More attention to types and their precise definitions
- More attention to memory ownership and lifetimes
- More attention to effect boundaries and purity
- More attention to proving properties hold

This is painful. When you're trying to ship a feature, fighting with a borrow checker or satisfying a type constraint feels like overhead. We humans want to "just make it work" and move on.

But here's the thing: **LLMs don't care about tedium**.

What's painful for us is trivial for them. An LLM can happily generate verbose type annotations, satisfy lifetime constraints, and write proof terms. It doesn't get frustrated. It doesn't cut corners because it's tired. The ceremony that makes safer languages feel heavy to humans is just more tokens to an LLM.

Type-safety is painful when you're migrating from unsafe to safe, or when you're creating software from scratch. But it's marvelous when you're maintaining, refactoring, testing, and explaining code. LLMs could handle the painful part while we reap the benefits.

## The twist: OCaml isn't safe enough

Here's where I have to be honest about my favorite language.

OCaml has an excellent type system. It catches a huge class of errors at compile time. Pattern matching is exhaustive. The module system is powerful. I love working in it.

But for the purpose of maximizing compiler feedback for LLMs, OCaml hits a ceiling. There are things you simply can't express in its type system:

- **Memory safety**: OCaml has a garbage collector, which helps, but it doesn't have Rust's guarantees about ownership and borrowing. You can still create logical races with mutable state.
- **Race-free data operations**: There's no ownership model preventing concurrent access to shared mutable data.
- **Effect tracking**: Side effects can happen anywhere. The compiler doesn't know which functions do I/O and which are pure.
- **Dependent types**: You can't encode complex invariants in types. The type system, while strong, can't prove arbitrary properties about your code.

The more properties you can encode in your types and modules, the more the compiler can verify. OCaml is safer than Python, but it's not safe enough to give LLMs the comprehensive feedback they could benefit from.

## The ideal language doesn't exist yet

What would the perfect language for LLM-assisted development look like? I think it's a combination that doesn't quite exist:

**Rust's memory safety**: Ownership and borrowing, compile-time guarantees about memory access, no data races.

**Haskell's purity**: Effects tracked in types. The compiler knows exactly which functions can do I/O, modify state, or throw exceptions.

**Lean's dependent types**: Express arbitrary properties in your types. Prove that your sorting function actually sorts, that your parser handles all inputs, that your state machine only allows valid transitions.

Some languages are moving in this direction. Idris combines dependent types with effect tracking. Koka has algebraic effects. Rust keeps adding more expressive type features. But we're not there yet.

Until that ideal language emerges, the direction is clear: **choose languages that maximize what the compiler can verify**. The more your compiler can catch, the more useful it becomes as a feedback mechanism for AI-assisted development.

## The future might be weirder than we think

Here's my speculation: LLMs might shift which languages succeed, but not in the way most people expect.

The conventional wisdom is that LLMs will entrench Python and JavaScript because that's what they know best. And in the short term, that's probably true.

But in the long term, the economics might flip. If LLMs can handle the tedium of safer languages, and if those languages provide better feedback loops, then the "difficult" languages become the productive ones. The languages humans find annoying might be exactly what LLMs prefer.

Imagine a future where you describe what you want in natural language, and an LLM writes Lean code that proves its own correctness. The code is verbose, heavily annotated, full of proof terms. You would never want to write it by hand. But you don't have to. The LLM handles the ceremony, and you get mathematically verified software.

That future isn't here yet. Today's LLMs [still get stuck in loops](https://zed.dev/blog/why-llms-cant-build-software), repeat mistakes, and need manual intervention. The safer languages have smaller communities and less training data. But the trajectory seems clear.

Maybe we should stop asking "what languages are LLMs good at?" and start asking "what languages would make LLMs better?" The answer might not be Python.
