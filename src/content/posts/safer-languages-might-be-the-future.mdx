---
title: "Safer languages might be the future of AI-driven development"
publishedAt: "2025-10-12"
isDraft: true
tags:
  - "AI"
  - "LLMs"
  - "Programming Languages"
  - "Type Safety"
description: "Why LLMs might benefit from maximally-safe programming languages, and why even OCaml isn't safe enough"
---

The rapid evolution of LLMs is reshaping software development. We see them assisting in code generation, debugging, and even architectural design. But there's something odd happening: ask an LLM to solve a problem (without specifying the language), and it almost always reaches for Python or JavaScript/TypeScript.

This isn't surprising. As [Kiran Gopinathan notes](https://kirancodes.me/posts/log-lang-design-llms.html), LLMs have substantially higher efficacy when operating in languages well-represented in their training data.

Python and JavaScript are the most used programming languages, so LLMs know them best. But here's the question that's been nagging me: **is this the optimal direction?**

Both languages were designed with a similar philosophy: remove friction, let developers move fast. Python, prioritized readability and simplicity over strictness. JavaScript, needed to be approachable enough that anyone could add interactivity to a webpage.

This permissiveness is exactly why they spread everywhere. Low barrier to entry, instant feedback, no compilation step. You write code, run and see the result. Python became the language of scripting, then data science, then machine learning. JavaScript became the only language that runs natively in browsers, making it unavoidable for web development.

In this blog post I want to reconsider the languages we choose in this new era. This may not be about LLMs adapting to our current paradigms, and whether certain language characteristics make them more suitable for AI-driven development.

I'll spoil it upfront: even as a bit OCaml fan, I've come to believe it might not be the best choice neither. Mostly because it's **not safe enough**.

## The shift was already happening

Before LLMs entered the picture, the industry was already moving toward safer languages. This wasn't a coordinated effort. It was a response to a painful situation: as software grows in complexity, the cost of fixing bugs scales dramatically. [Microsoft found that 70% of their CVEs are memory safety issues](https://www.microsoft.com/en-us/msrc/blog/2019/07/we-need-a-safer-systems-programming-language). You fix one bug and four pop up.

Developers and organizations recognized that languages preventing entire classes of errors at compile-time lead to more maintainable, secure, and reliable software. This proactive approach to error prevention was gaining momentum independently in a lot of ecosystems:

- **TypeScript** brought gradual static typing to JavaScript, and it's now the default for serious frontend work
- **Rust** gained significant traction for systems programming, offering memory safety without a garbage collector and robust concurrency guarantees
- **Elixir** added type system enhancements, bolstering its reputation for fault-tolerant systems
- **Gleam** emerged with strong static typing for the Erlang VM and JavaScript runtimes
- Even **Python** now has type hints baked into the language

## Compilers as feedback loop

Here's the core insight: **compilers provide instant, deterministic feedback**.

In Python, you run code and maybe it crashes. Maybe it silently does the wrong thing. You might not discover the bug until production.

In a language with a strong type system, the compiler tells you immediately: "this is logically inconsistent."

A strict compiler (or even a strict linter) can guide an LLM toward correctness with every iteration. My thesis is: the safer the language, the more the compiler can verify, the faster the feedback loop, and the more reliably the LLM can converge on correct code. [Research backs this up](https://arxiv.org/abs/2403.16792): compiler feedback can improve LLM accuracy by over 80%.

I now include a rule in all my projects: "Never accept broken compiler output. Every type mismatch must be fixed. When in doubt, ask me." and is working extremly well.

## The safety spectrum

I mentioned "safety" in a lot of places, but what does even mean?

Not all safety is equal. [Different languages verify different properties](https://cdsmith.wordpress.com/2011/01/09/an-old-article-i-wrote/). Think of it as a spectrum of "what can the compiler can know from your code?"

**Type safety** ensures operations are valid for the data they operate on. Languages like OCaml and Gleam excel here, their compiler enforces contracts between functions, makes illegal states unrepresentable, and guarantees exhaustive pattern matching.

**Memory safety** catches buffer overflows, use-after-free errors, and dangling pointers. Rust is the gold standard, using ownership and borrowing to guarantee memory safety at compile time without a garbage collector.

**Thread safety** catches race conditions and data races. Again, Rust leads here with its ownership model preventing concurrent mutable access. Worth mention for OxCaml.

**Effect tracking** catches side effects. Haskell forces you to encode I/O operations in types, making it impossible to accidentally perform side effects in pure functions.

**Dependent types** catch logical properties. Even not being often classified as a safety feature, some languages like Idris and Lean let you express invariants in your types, proving properties about your code mathematically.

Each level up this spectrum means more properties the compiler can verify. More verification means tighter feedback loops for LLMs. An LLM working with a dependently-typed language could, in theory, prove its code correct rather than just hoping it works.

## The paradox: tedious for humans, perfect for machines

If safer languages are so powerful, why aren't we all writing Lean or Idris?

Because **safer languages are tedious to write**. They demand more attention:

- More learning curve
- More attention to types and their precise definitions
- More attention to memory ownership and lifetimes
- More attention to effect boundaries and purity
- More attention to proving properties hold

This is painful. When you're trying to ship a feature, fighting with a borrow checker or satisfying a type constraint feels like overhead. Sometimes as humans we want to "just make it work".

But here's the thing: **LLMs don't care about tedium**.

What's painful for us is trivial for them. An LLM can happily generate verbose type annotations, satisfy lifetime constraints, and write proof terms. It doesn't get frustrated. It doesn't cut corners because it's tired. The ceremony that makes safer languages feel heavy to humans is just more tokens to an LLM.

The situation where we found type-safety being painful is when you're migrating from unsafe to safe, or maybe when you're creating software from scratch. But it's marvelous when you're maintaining, refactoring, testing, and explaining code. LLMs could handle the painful part while we reap the benefits.

## Most languages aren't safe enough

Here's where I have to be honest about my favorite language.

OCaml has an excellent type system. It catches a huge class of errors at compile time. Pattern matching is exhaustive. The module system is powerful. I love working in it.

But for the purpose of maximizing compiler feedback for LLMs, OCaml hits a ceiling. There are things you simply can't express in its type system:

- **Memory safety**: OCaml has a garbage collector, which helps, but it doesn't have Rust's guarantees about ownership and borrowing. You can still create logical races with mutable state.
- **Race-free data operations**: There's no ownership model preventing concurrent access to shared mutable data.
- **Effect tracking**: Side effects can happen anywhere. The compiler doesn't know which functions do I/O and which are pure.
- **Dependent types**: You can't encode complex invariants in types. The type system, while strong, can't prove arbitrary properties about your code.

## But ecosystems matter

In TypeScript land, you can probably set up a Supabase database, BetterAuth authentication, Stripe payments, Twilio SMS, and Sentry error tracking in a single prompt. The LLM knows these APIs inside out. It's seen thousands of examples.

Now imagine doing the same in Rust, or even Lean. The LLM would struggle, not because the language is harder, but because there's less training data, fewer examples, smaller ecosystems. The libraries might not even exist.

This is the real barrier. Safer languages don't just need better compilers. They need ecosystems rich enough that LLMs can learn from them. Today, TypeScript wins not because it's the safest choice, but because it's the most documented one.

But again, I doubt the barrier of creating those nice abstractions is what's stopping us from choosing another "better" language (and sorry for the better, there's no better one!).

## The ideal language doesn't exist yet

What would the perfect language for LLM-assisted development look like? I think it's a combination that doesn't quite exist, let me enumerate a bunch of features:

**Rust's memory safety**: Ownership and borrowing, compile-time guarantees about memory access, no data races.

**Haskell's purity**: Effects tracked in types. The compiler knows exactly which functions can do I/O, modify state, or throw exceptions.

**Lean's dependent types**: Express arbitrary properties in your types. Prove that your sorting function actually sorts, that your parser handles all inputs, that your state machine only allows valid transitions.

Some languages are moving in this direction. Idris combines dependent types with effect tracking. Koka has algebraic effects. Rust keeps adding more expressive type features. But we're not there yet.

Research is already exploring this space. [CoCoGen](https://arxiv.org/abs/2403.16792) shows that compiler feedback can improve LLM code accuracy by over 80%. [VeCoGen](https://arxiv.org/abs/2411.19275) combines LLMs with formal verification, iteratively refining code until it satisfies specifications. [TheoremLlama](https://arxiv.org/abs/2407.03203) trains LLMs to write Lean proofs, using the type checker as a feedback mechanism.

We're in early days. The language that combines all these properties in a way that's practical for LLM-assisted development doesn't exist yet. But someone will build it.

## The future might be weirder than we think

Here's my speculation: LLMs might shift which languages succeed, but not in the way most people expect.

The conventional wisdom is that LLMs will entrench Python and JavaScript because that's what they know best. And in the short term, that's probably true.

But in the long term, the economics might flip. If LLMs can handle the tedium of safer languages, and if those languages provide better feedback loops, then the "difficult" languages become the productive ones. The languages humans find annoying might be exactly what LLMs prefer.

Imagine a future where you describe what you want in natural language, and an LLM writes Lean code that proves its own correctness. The code is verbose, heavily annotated, full of proof terms. You would never want to write it by hand. But you don't have to. The LLM handles the ceremony, and you get mathematically verified software.

That future isn't here yet. Today's LLMs [still get stuck in loops](https://zed.dev/blog/why-llms-cant-build-software), repeat mistakes, and need manual intervention. The safer languages have smaller communities and less training data. But the trajectory seems clear.

Maybe we should stop asking "what languages are LLMs good at?" and start asking "what languages would make LLMs better?" The answer might not be Python.
